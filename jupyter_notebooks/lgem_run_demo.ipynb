{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ec98ff",
   "metadata": {},
   "source": [
    "# LGEM demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7fef9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import os\n",
    "\n",
    "from gears import PertData\n",
    "\n",
    "# own imports\n",
    "from lgem.models import (\n",
    "    LinearGeneExpressionModelLearned,\n",
    "    LinearGeneExpressionModelOptimized,\n",
    ")\n",
    "from lgem.test import test as test_lgem\n",
    "from lgem.train import train as train_lgem\n",
    "from lgem.data import pseudobulk_data_per_perturbation\n",
    "from data_utils.single_norman_utils import separate_data, get_common_genes\n",
    "\n",
    "from lgem.utils import predict_evaluate_lgem_double"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72c8552",
   "metadata": {},
   "source": [
    "#### Some relevant code to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c8b3d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings_double(\n",
    "    Y: torch.Tensor,  # noqa: N803\n",
    "    perts: List[str], # all perturbations, single and double\n",
    "    genes: List[str],\n",
    "    d_embed: int = 10,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Compute gene and perturbation embeddings.\n",
    "\n",
    "    Args:\n",
    "        Y: Data matrix with shape (n_genes, n_perturbations).\n",
    "        perts: List of perturbations.\n",
    "        genes: List of genes.\n",
    "        d_embed: Embedding dimension.\n",
    "\n",
    "    Returns:\n",
    "        G: Gene embedding matrix with shape (n_genes, d_embed).\n",
    "        P: Perturbation embedding matrix with shape (n_perturbations, d_embed).\n",
    "        b: Bias vector with shape (n_genes).\n",
    "    \"\"\"\n",
    "    # Perform a PCA on Y to obtain the top d_embed principal components, which will\n",
    "    # serve as the gene embeddings G.\n",
    "    pca = PCA(n_components=d_embed)\n",
    "    G = pca.fit_transform(Y)  # noqa: N806\n",
    "\n",
    "    gene_to_idx = {gene: i for i, gene in enumerate(genes)}\n",
    "    gene_to_emb = {gene: G[i] for gene, i in gene_to_idx.items()}\n",
    "\n",
    "    P = []\n",
    "    missing = []\n",
    "    for pert in perts:\n",
    "        genes_in_pert = pert.split(\"+\")\n",
    "        try:\n",
    "            emb_list = [gene_to_emb[g] for g in genes_in_pert]\n",
    "            pert_emb = np.mean(emb_list, axis=0)  # average embeddings\n",
    "            P.append(pert_emb)\n",
    "        except KeyError as e:\n",
    "            missing.append(pert)\n",
    "            P.append(np.zeros(d_embed))  # fallback if gene not found\n",
    "\n",
    "    P = np.array(P)\n",
    "    if missing:\n",
    "        print(f\"{len(missing)}/{len(perts)} missing embeddings.\")\n",
    "        print(f\"Missing embeddings for perturbations: {missing}\")\n",
    "\n",
    "    # Compute b as the average expression of each gene across all perturbations.\n",
    "    b = Y.mean(axis=1)\n",
    "\n",
    "    return torch.from_numpy(G).float(), torch.from_numpy(P).float(), b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cb0db1",
   "metadata": {},
   "source": [
    "## LGEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d3fd399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "criterion = nn.MSELoss()\n",
    "name = 'demo_test' # Name of project\n",
    "savedir='/workspace/tfm/cris_test/models/'\n",
    "eval_dir = '/workspace/tfm/cris_test/results/'\n",
    "data_dir = '/workspace/tfm/cris_test/data'\n",
    "\n",
    "eval_dir = os.path.join(eval_dir, name)\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "savedir = os.path.join(savedir, name)\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "dataset_name='norman_reduced'\n",
    "prediction_type='double'\n",
    "seed=1\n",
    "num_runs=1\n",
    "n_epochs=200\n",
    "batch_size=8\n",
    "top_deg=20 # Top Differentially expressed genes to evalaute from\n",
    "\n",
    "run_name = f\"lgem_{dataset_name}_epochs_{n_epochs}_batch_{batch_size}\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Make results file path.\n",
    "results_file_path = os.path.join(\n",
    "    eval_dir, f\"{run_name}_train_test_metrics.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89074bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from /workspace/tfm/cris_test/data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "These perturbations are not in the GO graph and their perturbation can thus not be predicted\n",
      "['LYL1+IER5L' 'IER5L+ctrl' 'KIAA1804+ctrl' 'ctrl+IER5L']\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(f\"Loading dataset from {data_dir}.\")\n",
    "datahandler = PertData(data_dir)\n",
    "datahandler.load(data_path = os.path.join(data_dir, dataset_name)) # load the processed data, the path is saved folder + dataset_name\n",
    "\n",
    "pertdata = datahandler.adata\n",
    "# pertdata.prepare_split(split = 'simulation', seed = 42) # get data split with seed\n",
    "# pertdata.get_dataloader(batch_size = 32, test_batch_size = 128) # prepare data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a20f08cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "View of AnnData object with n_obs × n_vars = 11700 × 5000\n",
       "    obs: 'guide_identity', 'read_count', 'UMI_count', 'gemgroup', 'good_coverage', 'number_of_cells', 'guide_ids', 'guide_merged', 'split', 'batch', 'condition', 'cell_type', 'dose_val', 'control', 'drug_dose_name', 'cov_drug_dose_name', 'condition_name', 'condition_fixed'\n",
       "    var: 'gene_symbols', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'gene_name'\n",
       "    uns: 'non_dropout_gene_idx', 'non_zeros_gene_idx', 'rank_genes_groups', 'rank_genes_groups_cov', 'rank_genes_groups_cov_all', 'top_non_dropout_de_20', 'top_non_zero_de_20'\n",
       "    layers: 'counts'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pertdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b678c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique perturbations: 125/11650\n"
     ]
    }
   ],
   "source": [
    "# Get separate dataset for single, double perts and controls\n",
    "# Get resulting RNA-seq dataset with valid samples (sample pert genes are genes found in features)\n",
    "pertdata_single, pertdata_double, pertdata_ctrl = separate_data(adata = pertdata, dataset_name = dataset_name)\n",
    "\n",
    "if prediction_type == \"double\":\n",
    "    # Join both AnnData datasets\n",
    "    pertdata_both = pertdata_single.concatenate(pertdata_double, join = 'outer', index_unique = '-')\n",
    "    all_perts, perts, genes, pertdata_common = get_common_genes(adata = pertdata_both, dataset_name = dataset_name)\n",
    "else:\n",
    "    # Y, perts and genes\n",
    "    all_perts, perts, genes, pertdata_common = get_common_genes(adata_single = pertdata_single, dataset_name = dataset_name)    \n",
    "\n",
    "print(f\"Number of unique perturbations: {len(perts)}/{len(all_perts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a72d1ce",
   "metadata": {},
   "source": [
    "- lgem works with individual samples for each condition\n",
    "- Ordered perturbations equivalent to Y is in perts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1263b033",
   "metadata": {},
   "source": [
    "This model:\n",
    "- Separates single gene perturbation samples from double perturbation samples\n",
    "- single perts: split into train and validation\n",
    "- double perts: all of them used as test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a422cef",
   "metadata": {},
   "source": [
    "### Training lgem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90386613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pseudobulking:   0%|                                                                                               | 0/125 [00:00<?, ?perturbation/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pseudobulking: 100%|████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:01<00:00, 111.73perturbation/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss (optimized model) | Run 1: 0.0030\n",
      "Val loss (optimized model): 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████| 200/200 [04:07<00:00,  1.24s/epoch, Training Loss: 1.2132 | Validation Loss: 6.0498]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss (optimized model) | Run 1: 0.0030\n",
      "Val loss (learned model): 6.0498\n"
     ]
    }
   ],
   "source": [
    "Y = pseudobulk_data_per_perturbation(perts, genes, pertdata_common)\n",
    "\n",
    "# Compute the embeddings on the entire dataset (with singles and doubles)\n",
    "G, P, b = compute_embeddings_double(Y.T, perts, genes)  # noqa: N806\n",
    "\n",
    "# Keeping only embedding related to single perturbations for training\n",
    "singles_idx = [i for i, pert in enumerate(perts) if \"+\" not in pert]\n",
    "sY = Y[singles_idx, :]\n",
    "sP = P[singles_idx, :]\n",
    "\n",
    "with open(file=results_file_path, mode=\"w\") as f:\n",
    "    print(\n",
    "        \"seed,optimized_train_loss,optimized_model_loss,learned_train_loss,learned_model_loss\",\n",
    "        file=f,\n",
    "    )\n",
    "\n",
    "    # Setting up several runs (for this example, only 1)\n",
    "    for current_run in range(num_runs):\n",
    "        current_seed = seed + current_run\n",
    "        torch.manual_seed(current_seed)\n",
    "        model_name = f\"lgem_{dataset_name}_seed_{current_seed}_epoch_{n_epochs}_batch_{batch_size}\"\n",
    "\n",
    "        # Directory for custom name for model where more pickle files will be saved\n",
    "        # savedir = args.savedir\n",
    "        current_savedir = os.path.join(savedir, model_name)\n",
    "        os.makedirs(current_savedir, exist_ok=True)\n",
    "\n",
    "\n",
    "# If prediction type is not double, splitter only splits into train and test\n",
    "# Y, P used\n",
    "        if prediction_type != \"double\":\n",
    "            # Split the data into training and test sets and create the dataloaders.\n",
    "            Y_train, Y_test, P_train, P_test = train_test_split(  # noqa: N806\n",
    "                Y, P, test_size=0.2, random_state=current_seed\n",
    "            )\n",
    "            # Get indices of perturbations in the train/test sets\n",
    "            train_indices, test_indices = train_test_split(range(len(perts)), test_size=0.2, random_state=42)\n",
    "            # Get the equivalent perturbations for the training and test sets\n",
    "            perts_train = [perts[i] for i in train_indices]\n",
    "            perts_test = [perts[i] for i in test_indices]\n",
    "\n",
    "            # Dataloaders\n",
    "            train_dataset = TensorDataset(P_train, Y_train)\n",
    "            test_dataset = TensorDataset(P_test, Y_test)\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            # # Saving dataloaders as pickles and perturbation order from perts\n",
    "            # torch.save(train_dataloader, os.path.join(current_savedir, \"train_dataloader.pt\"))\n",
    "            # torch.save(test_dataloader, os.path.join(current_savedir, \"train_dataloader.pt\"))\n",
    "            # torch.save({\"perts_train\": perts_train,\n",
    "            #             \"perts_test\": perts_test},\n",
    "            #             os.path.join(current_savedir, \"perts.pt\"))\n",
    "            # val_dataloader = None\n",
    "\n",
    "# If prediction is DOUBLE, train and valdation are taken from single pert sampels\n",
    "# sY, sP (previously separated) are used\n",
    "        else:\n",
    "            # Keep train and val dataloaders for training\n",
    "            Y_train, Y_val, P_train, P_val = train_test_split(  # noqa: N806\n",
    "                sY, sP, test_size=0.2, random_state=current_seed\n",
    "            )\n",
    "\n",
    "            # Dataloaders\n",
    "            train_dataset = TensorDataset(P_train, Y_train)\n",
    "            val_dataset = TensorDataset(P_val, Y_val)\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "            # Test\n",
    "            # Get indices of perturbations in the train/test set\n",
    "            doubles_idx = [i for i, pert in enumerate(perts) if \"+\" in pert]\n",
    "            doubles_dataset = TensorDataset(P[doubles_idx, :], Y[doubles_idx, :])\n",
    "            test_dataloader = DataLoader(doubles_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Save dataloaders as pickles and perturbation order from perts\n",
    "            # torch.save(train_dataloader, os.path.join(current_savedir, \"train_dataloader.pt\"))\n",
    "            # torch.save(val_dataloader, os.path.join(current_savedir, \"val_dataloader.pt\"))\n",
    "            # torch.save(test_dataloader, os.path.join(current_savedir, \"test_dataloader.pt\")) # equivalent to doubles only\n",
    "            # torch.save({\"perts\": perts}, os.path.join(current_savedir, \"perts.pt\"))\n",
    "\n",
    "        # Fit the OPTIMIZED model to the training data.\n",
    "        model_optimized = LinearGeneExpressionModelOptimized(Y_train.T, G, P_train, b)\n",
    "        train_loss_op = test_lgem(model_optimized, criterion, train_dataloader, device)\n",
    "        print(f\"Train loss (optimized model) | Run {current_run+1}: {train_loss_op:.4f}\")\n",
    "        test_loss_op = 0\n",
    "\n",
    "        if prediction_type != \"double\":\n",
    "            # Test the optimized model.\n",
    "            test_loss_op = test_lgem(model_optimized, criterion, test_dataloader, device)\n",
    "            print(f\"Val loss (optimized model): {test_loss_op:.4f}\")\n",
    "        else:\n",
    "            # Test the optimized model on validation set\n",
    "            test_loss_op = test_lgem(model_optimized, criterion, val_dataloader, device)\n",
    "            validation=True\n",
    "            print(f\"Val loss (optimized model): {test_loss_op:.4f}\")\n",
    "\n",
    "        # Fit the LEARNED model to the training data.\n",
    "        model_learned = LinearGeneExpressionModelLearned(G, b)\n",
    "        optimizer = torch.optim.Adam(params=model_learned.parameters(), lr=1e-3)\n",
    "        model_learned = train_lgem(\n",
    "            model_learned, criterion, optimizer, train_dataloader, val_dataloader, n_epochs, device, validation=validation\n",
    "        )\n",
    "        train_loss_learn = test_lgem(model_optimized, criterion, val_dataloader, device)\n",
    "        print(f\"Train loss (optimized model) | Run {current_run+1}: {train_loss_op:.4f}\")\n",
    "        test_loss_learn = 0\n",
    "\n",
    "        if prediction_type != \"double\":\n",
    "            # Test the learned model.\n",
    "            test_loss_learn = test_lgem(model_learned, criterion, test_dataloader, device)\n",
    "            print(\"Val loss (learned model): {test_loss_learn:.4f}\")  \n",
    "        else:\n",
    "            test_loss_learn = test_lgem(model_learned, criterion, val_dataloader, device)\n",
    "            print(f\"Val loss (learned model): {test_loss_learn:.4f}\")\n",
    "\n",
    "        # Save results to file\n",
    "        print(f\"{current_seed},{train_loss_op}, {test_loss_op},{train_loss_learn}, {test_loss_learn}\",\n",
    "            file=f,\n",
    "        )\n",
    "\n",
    "        # Save models and embeddings\n",
    "        # torch.save(model_optimized.state_dict(), os.path.join(current_savedir, \"optimized_best_model.pt\"))\n",
    "        # torch.save(model_learned.state_dict(), os.path.join(current_savedir, \"learned_best_model.pt\"))\n",
    "        # torch.save(G, os.path.join(current_savedir, \"G.pt\"))\n",
    "        # torch.save(b, os.path.join(current_savedir, \"b.pt\"))\n",
    "        # torch.save(P_train, os.path.join(current_savedir, \"P.pt\"))\n",
    "        # torch.save(Y_train, os.path.join(current_savedir, \"Y.pt\"))\n",
    "        # print(f\"Saved models and embedding at {current_savedir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd7635d",
   "metadata": {},
   "source": [
    "### Testing on doubles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96fa5099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables and parameters\n",
    "# In script it's usually loaded from previous configurations and saved files\n",
    "\n",
    "# Trained model path\n",
    "current_seed = seed\n",
    "model_name = f\"lgem_{dataset_name}_seed_{current_seed}_epoch_{n_epochs}_batch_{batch_size}\"\n",
    "current_savedir = os.path.join(savedir, model_name)\n",
    "\n",
    "# Perturbation list\n",
    "perturbation_list = perts\n",
    "\n",
    "# Embeddings\n",
    "G = G\n",
    "b = b\n",
    "P = P_train\n",
    "Y = Y_train\n",
    "\n",
    "# # Load dataset (dataloader)\n",
    "# test_dataloader = torch.load(os.path.join(savedir, \"test_dataloader.pt\"))\n",
    "# perturbation_list = torch.load(os.path.join(savedir, \"perts.pt\"))\n",
    "# perturbation_list = perturbation_list[\"perts\"]\n",
    "\n",
    "# Load Embeddings\n",
    "# G = torch.load(os.path.join(savedir, \"G.pt\"))\n",
    "# P = torch.load(os.path.join(savedir, \"P.pt\"))\n",
    "# Y = torch.load(os.path.join(savedir, \"Y.pt\"))\n",
    "# b = torch.load(os.path.join(savedir, \"b.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e960346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting and calculating loss for double perturbations.\n"
     ]
    }
   ],
   "source": [
    "# Optimized model\n",
    "model_optimized = LinearGeneExpressionModelOptimized(Y.T, G, P, b)\n",
    "# model_optimized.load_state_dict(torch.load(os.path.join(savedir, \"optimized_best_model.pt\")))\n",
    "double_perts_list_op, double_predictions_op, ground_truth, mse_pred_op = predict_evaluate_lgem_double(model_optimized, device, test_dataloader, perturbation_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af718237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting and calculating loss for double perturbations.\n"
     ]
    }
   ],
   "source": [
    "# Learned model\n",
    "model_learned = LinearGeneExpressionModelLearned(G, b)\n",
    "# model_learned.load_state_dict(torch.load(os.path.join(savedir, \"learned_best_model.pt\")))\n",
    "_, double_predictions_learn, _, mse_pred_learn= predict_evaluate_lgem_double(model_learned, device, test_dataloader, perturbation_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07a70030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE calculation done.\n"
     ]
    }
   ],
   "source": [
    "# Randomly chosen control cells for baseline\n",
    "rand_idx = np.random.randint(low=0, high=pertdata_ctrl.X.shape[0], size=len(double_perts_list_op))\n",
    "baseline_control = pertdata_ctrl.X[rand_idx, :].toarray()\n",
    "\n",
    "# Turning profiles into arrays\n",
    "double_predictions_op = np.asarray(double_predictions_op)\n",
    "double_predictions_learn = np.asarray(double_predictions_learn)\n",
    "gt = np.asarray(ground_truth)\n",
    "baseline_control = np.asarray(baseline_control) # Redundant I tink but it broke\n",
    "\n",
    "\n",
    "# Calculate true - pred MSE\n",
    "mse_control_op = np.mean((gt - baseline_control) ** 2, axis = 1)\n",
    "mse_control_learn = np.mean((gt - baseline_control) ** 2, axis = 1)\n",
    "print(\"MSE calculation done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f62e4501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson calculation done\n"
     ]
    }
   ],
   "source": [
    "# Calculate Pearson correlation\n",
    "gt_deg = gt - baseline_control\n",
    "deg_idx = np.argsort(abs(gt_deg), axis=1)[:, -top_deg:]\n",
    "\n",
    "# Select values along the top DEG indices for each sample\n",
    "pred_op_selected = np.take_along_axis(double_predictions_op - baseline_control, deg_idx, axis=1)\n",
    "pred_learn_selected = np.take_along_axis(double_predictions_learn - baseline_control, deg_idx, axis=1)\n",
    "gt_selected = np.take_along_axis(gt_deg, deg_idx, axis=1)\n",
    "\n",
    "\n",
    "pearson_op = np.array([pearsonr(pred_op_selected[i], gt_selected[i])\n",
    "                       for i in range(pred_op_selected.shape[0])\n",
    "                       ])\n",
    "pearson_learn = np.array([pearsonr(pred_learn_selected[i], gt_selected[i])\n",
    "                       for i in range(pred_learn_selected.shape[0])\n",
    "                       ])\n",
    "\n",
    "print(\"Pearson calculation done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f3a65218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics to result dir\n",
    "result_df = pd.DataFrame({\"double\": double_perts_list_op,\n",
    "                        \"mse_true_vs_control_op\": mse_control_op,\n",
    "                        \"mse_true_vs_control_learn\": mse_control_learn,\n",
    "                        \"mse_true_vs_pred_op\": mse_pred_op,\n",
    "                        \"mse_true_vs_pred_learn\": mse_pred_learn,\n",
    "                        \"pearson_op\": pearson_op[:, 0],\n",
    "                        \"pearson_op_pvalue\": pearson_op[:, 1],\n",
    "                        \"pearson_learn\": pearson_learn[:, 0],\n",
    "                        \"pearson_learn_pvalue\": pearson_learn[:, 1]})\n",
    "\n",
    "double_pred_op = pd.DataFrame(double_predictions_op, columns=pertdata_ctrl.var_names)\n",
    "double_pred_op.insert(0, 'double', double_perts_list_op)\n",
    "double_pred_learn = pd.DataFrame(double_predictions_learn, columns=pertdata_ctrl.var_names)\n",
    "double_pred_learn.insert(0, 'double', double_perts_list_op)\n",
    "\n",
    "\n",
    "# double_pred_op.to_csv(os.path.join(args.eval_dir, f\"{model_name}_double_predictions_op.csv\"), index=False)\n",
    "# double_pred_learn.to_csv(os.path.join(args.eval_dir, f\"{model_name}_double_predictions_learn.csv\"), index=False)\n",
    "# result_df.to_csv(os.path.join(args.eval_dir, f\"{model_name}_double_metrics.csv\"), index=False)\n",
    "# print(f\"Results saved to {os.path.join(args.eval_dir, f'{model_name}_double_metrics.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29d96aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>double</th>\n",
       "      <th>mse_true_vs_control_op</th>\n",
       "      <th>mse_true_vs_control_learn</th>\n",
       "      <th>mse_true_vs_pred_op</th>\n",
       "      <th>mse_true_vs_pred_learn</th>\n",
       "      <th>pearson_op</th>\n",
       "      <th>pearson_op_pvalue</th>\n",
       "      <th>pearson_learn</th>\n",
       "      <th>pearson_learn_pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CEBPE+RUNX1T1</td>\n",
       "      <td>0.027876</td>\n",
       "      <td>0.027876</td>\n",
       "      <td>0.002543</td>\n",
       "      <td>11.850765</td>\n",
       "      <td>0.963878</td>\n",
       "      <td>8.721589e-12</td>\n",
       "      <td>0.214407</td>\n",
       "      <td>0.364017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AHR+FEV</td>\n",
       "      <td>0.095165</td>\n",
       "      <td>0.095165</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>11.573503</td>\n",
       "      <td>0.968748</td>\n",
       "      <td>2.411408e-12</td>\n",
       "      <td>0.717365</td>\n",
       "      <td>0.000370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOXA1+HOXB9</td>\n",
       "      <td>0.037915</td>\n",
       "      <td>0.037915</td>\n",
       "      <td>0.005112</td>\n",
       "      <td>11.186444</td>\n",
       "      <td>0.980608</td>\n",
       "      <td>3.433014e-14</td>\n",
       "      <td>-0.043954</td>\n",
       "      <td>0.854015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ETS2+MAP7D1</td>\n",
       "      <td>0.020817</td>\n",
       "      <td>0.020817</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>11.542901</td>\n",
       "      <td>0.969774</td>\n",
       "      <td>1.792068e-12</td>\n",
       "      <td>0.154671</td>\n",
       "      <td>0.514978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOXA3+FOXF1</td>\n",
       "      <td>0.042748</td>\n",
       "      <td>0.042748</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>11.095216</td>\n",
       "      <td>0.986575</td>\n",
       "      <td>1.281330e-15</td>\n",
       "      <td>0.174350</td>\n",
       "      <td>0.462234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>OSR2+PTPN12</td>\n",
       "      <td>0.054968</td>\n",
       "      <td>0.054968</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>9.575863</td>\n",
       "      <td>0.996485</td>\n",
       "      <td>7.685355e-21</td>\n",
       "      <td>0.519939</td>\n",
       "      <td>0.018780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>FOSB+UBASH3B</td>\n",
       "      <td>0.034607</td>\n",
       "      <td>0.034607</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>23.589426</td>\n",
       "      <td>0.995367</td>\n",
       "      <td>9.187693e-20</td>\n",
       "      <td>-0.148715</td>\n",
       "      <td>0.531483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>ETS2+IGDCC3</td>\n",
       "      <td>0.025769</td>\n",
       "      <td>0.025769</td>\n",
       "      <td>0.002357</td>\n",
       "      <td>3.367628</td>\n",
       "      <td>0.988671</td>\n",
       "      <td>2.802764e-16</td>\n",
       "      <td>0.716451</td>\n",
       "      <td>0.000380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>TBX2+TBX3</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>0.005162</td>\n",
       "      <td>11.857831</td>\n",
       "      <td>0.829380</td>\n",
       "      <td>6.153504e-06</td>\n",
       "      <td>0.324285</td>\n",
       "      <td>0.163045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>DUSP9+ETS2</td>\n",
       "      <td>0.027759</td>\n",
       "      <td>0.027759</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>11.727517</td>\n",
       "      <td>0.946916</td>\n",
       "      <td>2.620110e-10</td>\n",
       "      <td>0.172302</td>\n",
       "      <td>0.467592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           double  mse_true_vs_control_op  mse_true_vs_control_learn  \\\n",
       "0   CEBPE+RUNX1T1                0.027876                   0.027876   \n",
       "1         AHR+FEV                0.095165                   0.095165   \n",
       "2     FOXA1+HOXB9                0.037915                   0.037915   \n",
       "3     ETS2+MAP7D1                0.020817                   0.020817   \n",
       "4     FOXA3+FOXF1                0.042748                   0.042748   \n",
       "..            ...                     ...                        ...   \n",
       "61    OSR2+PTPN12                0.054968                   0.054968   \n",
       "62   FOSB+UBASH3B                0.034607                   0.034607   \n",
       "63    ETS2+IGDCC3                0.025769                   0.025769   \n",
       "64      TBX2+TBX3                0.033200                   0.033200   \n",
       "65     DUSP9+ETS2                0.027759                   0.027759   \n",
       "\n",
       "    mse_true_vs_pred_op  mse_true_vs_pred_learn  pearson_op  \\\n",
       "0              0.002543               11.850765    0.963878   \n",
       "1              0.013285               11.573503    0.968748   \n",
       "2              0.005112               11.186444    0.980608   \n",
       "3              0.004105               11.542901    0.969774   \n",
       "4              0.002878               11.095216    0.986575   \n",
       "..                  ...                     ...         ...   \n",
       "61             0.001859                9.575863    0.996485   \n",
       "62             0.001474               23.589426    0.995367   \n",
       "63             0.002357                3.367628    0.988671   \n",
       "64             0.005162               11.857831    0.829380   \n",
       "65             0.002954               11.727517    0.946916   \n",
       "\n",
       "    pearson_op_pvalue  pearson_learn  pearson_learn_pvalue  \n",
       "0        8.721589e-12       0.214407              0.364017  \n",
       "1        2.411408e-12       0.717365              0.000370  \n",
       "2        3.433014e-14      -0.043954              0.854015  \n",
       "3        1.792068e-12       0.154671              0.514978  \n",
       "4        1.281330e-15       0.174350              0.462234  \n",
       "..                ...            ...                   ...  \n",
       "61       7.685355e-21       0.519939              0.018780  \n",
       "62       9.187693e-20      -0.148715              0.531483  \n",
       "63       2.802764e-16       0.716451              0.000380  \n",
       "64       6.153504e-06       0.324285              0.163045  \n",
       "65       2.620110e-10       0.172302              0.467592  \n",
       "\n",
       "[66 rows x 9 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6316551",
   "metadata": {},
   "source": [
    "For lgem, the following results are saved:\n",
    "- Mean predictions for learned model\n",
    "- Mean predictions for optimised model\n",
    "- Metrics for learned model\n",
    "- Metrics for optimised model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
