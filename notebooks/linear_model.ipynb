{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess the scRNA-seq Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show available datasets from the `pertdata` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets:\n",
      "  DixitRegev2016\n",
      "  NormanWeissman2019_filtered\n",
      "  ReplogleWeissman2022_K562_essential\n",
      "  ReplogleWeissman2022_rpe1\n",
      "  adamson\n",
      "  dixit\n",
      "  norman\n",
      "  replogle_k562_essential\n",
      "  replogle_rpe1_essential\n",
      "  wessel_dataset\n"
     ]
    }
   ],
   "source": [
    "import pertdata as pt\n",
    "\n",
    "datasets_dict = pt.datasets()\n",
    "print(\"Available datasets:\")\n",
    "for key in datasets_dict.keys():\n",
    "    print(f\"  {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the \"NormanWeissman2019_filtered\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already cached: /workspaces/transmet/cache/pertdata/NormanWeissman2019_filtered\n",
      "Loading: /workspaces/transmet/cache/pertdata/NormanWeissman2019_filtered/adata.h5ad\n",
      "PertDataset object\n",
      "    name: NormanWeissman2019_filtered\n",
      "    cache_dir_path: /workspaces/transmet/cache/pertdata\n",
      "    path: /workspaces/transmet/cache/pertdata/NormanWeissman2019_filtered\n",
      "    adata: AnnData object with n_obs ✕ n_vars = 111445 ✕ 33694\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "NormanWeissman2019_filtered = pt.PertDataset(\n",
    "    name=\"NormanWeissman2019_filtered\",\n",
    "    cache_dir_path=os.path.join(\"..\", \"cache\", \"pertdata\"),\n",
    ")\n",
    "print(NormanWeissman2019_filtered)\n",
    "adata = NormanWeissman2019_filtered.adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The expression matrix is in `adata.X` with cells as rows and genes as columns.\n",
    "- Perturbation labels are stored in `adata.obs[\"perturbation\"]`.\n",
    "- Gene names are in `adata.var_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique perturbations: 237\n",
      "Unique genes: 33694\n"
     ]
    }
   ],
   "source": [
    "perturbations = adata.obs[\"perturbation\"].unique()\n",
    "genes = adata.var_names\n",
    "print(f\"Unique perturbations: {len(perturbations)}\")\n",
    "print(f\"Unique genes: {len(genes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demultiplex `adata` for control/single/double perturbations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_control = adata[adata.obs[\"nperts\"] == 0].copy()\n",
    "adata_single = adata[adata.obs[\"nperts\"] == 1].copy()\n",
    "adata_double = adata[adata.obs[\"nperts\"] == 2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique control perturbations: 1\n",
      "Unique single perturbations: 105\n",
      "Unique double perturbations: 131\n"
     ]
    }
   ],
   "source": [
    "perturbations_control = adata_control.obs[\"perturbation\"].unique()\n",
    "perturbations_single = adata_single.obs[\"perturbation\"].unique()\n",
    "perturbations_double = adata_double.obs[\"perturbation\"].unique()\n",
    "\n",
    "print(f\"Unique control perturbations: {len(perturbations_control)}\")\n",
    "print(f\"Unique single perturbations: {len(perturbations_single)}\")\n",
    "print(f\"Unique double perturbations: {len(perturbations_double)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue working **with single-gene perturbations only**.\n",
    "Also, we remove perturbations that are not measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 3 perturbations: ['C3orf72', 'C19orf26', 'KIAA1804']\n",
      "Unique single filtered perturbations: 102\n"
     ]
    }
   ],
   "source": [
    "to_remove = []\n",
    "for perturbation in perturbations_single:\n",
    "    if perturbation not in adata_single.var_names:\n",
    "        to_remove.append(perturbation)\n",
    "\n",
    "print(f\"Removing {len(to_remove)} perturbations: {to_remove}\")\n",
    "\n",
    "adata_single_filtered = adata_single[\n",
    "    ~adata_single.obs[\"perturbation\"].isin(to_remove)\n",
    "].copy()\n",
    "\n",
    "perturbations_single_filtered = adata_single_filtered.obs[\"perturbation\"].unique()\n",
    "print(f\"Unique single filtered perturbations: {len(perturbations_single_filtered)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that the expression matrix is in a dense format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import issparse\n",
    "\n",
    "if issparse(adata_single_filtered):\n",
    "    adata_single_filtered.X = adata_single_filtered.X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate (pseudobulk) the single-cell data per perturbation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = []\n",
    "\n",
    "for perturbation in perturbations_single_filtered:\n",
    "    # Subset the data for the current perturbation.\n",
    "    cells = adata_single_filtered[\n",
    "        adata_single_filtered.obs[\"perturbation\"] == perturbation\n",
    "    ]\n",
    "\n",
    "    # Compute the average expressions across cells.\n",
    "    mean_expressions = cells.X.mean(axis=0)\n",
    "\n",
    "    # Append the mean expression to Y_train.\n",
    "    Y_train.append(mean_expressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert `Y_train` to a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Y_train = np.array(Y_train).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Linear Gene Expression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Linear Gene Expression Model (LGEM) by [Ahlmann-Eltze et al. (2024)](https://doi.org/10.1101/2024.09.16.613342), we have:\n",
    "- Embeddings for read-out genes: $G$ (an $n \\times K$ matrix, where $n$ is the number of read-out genes and $K$ is the dimensionality of the embeddings).\n",
    "- Embeddings for perturbations: $P$ (an $m \\times K$ matrix, where $m$ is the number of perturbations).\n",
    "\n",
    "Given a data matrix $Y_{\\text{train}}$ of gene expression values, the model then fits the matrix $W$ by minimizing:\n",
    "$$\n",
    "\\arg\\min_{W} \\| Y_{\\text{train}} - (G W P^\\top + b) \\|^2\n",
    "$$\n",
    "\n",
    "Hence, we furthermore have:\n",
    "- Data matrix: $Y_{\\text{train}}$ (an $n \\times m$ matrix, i.e., pseudobulked per condition/perturbation).\n",
    "- Weight matrix: $W$ (a $K \\times K$ matrix to be learned).\n",
    "- Bias vector: $b$ (an $n \\times 1$ vector of average gene expressions).\n",
    "\n",
    "The model then predicts gene expression values using:\n",
    "$$\n",
    "Y_{\\text{train}} \\approx G W P^\\top + b\n",
    "$$\n",
    "\n",
    "Note: The bias vector $b$ (with dimensions $n \\times 1$) is added to each column of the matrix $G W P^\\top$ (with dimensions $n \\times m$).\n",
    "This operation effectively _broadcasts_ the vector $b$ across all $m$ columns, repeating it $m$ times to match the dimensions of $G W P^\\top$.\n",
    "PyTorch handles broadcasting automatically implicitly.\n",
    "\n",
    "In the paper, to obtain the embeddings $G$ and $P$, they performed a PCA on $Y_{\\text{train}}$ and used the top $K$ principal components for $G$.\n",
    "They then subset this $G$ to only the rows corresponding to genes that have been perturbed in the training data (and hence appear as columns in $Y_{\\text{train}}$) and used the resulting matrix for $P$.\n",
    "When using the model for prediction, they replace $P$ with the matrix formed by the rows of $G$ corresponding to genes perturbed in the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the expression matrix $Y_{\\text{train}}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_copy = Y_train.copy()\n",
    "Y_train = Y_train.T  # Shape: (n_genes, n_perturbations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a PCA on $Y_{\\text{train}}$ to obtain the top $K$ principal components, which will serve as the gene embeddings $G$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "K = 10\n",
    "pca = PCA(n_components=K)\n",
    "G = pca.fit_transform(Y_train)  # Shape: (n_genes, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract perturbation embeddings $P$ from $G$ by subsetting $G$ to only the rows corresponding to genes that have been perturbed in the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the indexes of perturbations.\n",
    "indexes = [\n",
    "    list(adata_single_filtered.var_names).index(gene_name)\n",
    "    for gene_name in perturbations_single_filtered\n",
    "]\n",
    "\n",
    "# Extract P from G.\n",
    "P = G[indexes, :]  # Shape: (n_perturbations, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $b$ as the average expression of each gene across all perturbations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Y_train.mean(axis=1, keepdims=True)  # Shape: (n_genes, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Center $Y_{\\text{train}}$ by subtracting $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_centered = Y_train - b  # Shape: (n_genes, n_perturbations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can vectorize the equation $Y_{\\text{centered}} = G W P^\\top$ and set it up in a form suitable for least squares.\n",
    "\n",
    "Using the mixed-product property of the Kronecker product, we have:\n",
    "$$\n",
    "\\text{vec} (Y_{\\text{centered}}) = (P \\otimes G) \\text{vec} (W)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $\\text{vec} (Y_{\\text{centered}})$ is the vectorization of $Y_{\\text{centered}}$ (flattened column-wise).\n",
    "- $P \\otimes G$ is the Kronecker product of $P$ and $G$.\n",
    "- $\\text{vec} (W)$ is the vectorization of $W$ (flattened column-wise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import kron\n",
    "\n",
    "# Flatten Y_centered.\n",
    "Y_vec = Y_centered.flatten(order=\"F\")\n",
    "\n",
    "# Ensure that P and G are numpy arrays.\n",
    "P = np.array(P)\n",
    "G = np.array(G)\n",
    "\n",
    "# Compute the Kronecker product of P and G.\n",
    "Kron_P_G = kron(P, G)  # Shape: (n_genes * n_perturbations, K * K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now solve the linear equation:\n",
    "$$\n",
    "Y_{\\text{vec}} = (P \\otimes G) \\cdot W_{\\text{vec}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import lstsq\n",
    "\n",
    "# Solve the least squares problem.\n",
    "W_vec, residuals, rank, s = lstsq(Kron_P_G, Y_vec, rcond=None)\n",
    "\n",
    "# Reshape vec(W) back to matrix W.\n",
    "W = W_vec.reshape(K, K, order=\"F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.4264335036277771\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct Y_centered using G, W, and P.\n",
    "Y_centered_pred = G @ W @ P.T  # Shape: (n_genes, n_perturbations)\n",
    "\n",
    "# Compute the mean squared error.\n",
    "mse = np.mean((Y_centered - Y_centered_pred) ** 2)\n",
    "print(f\"MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulating the Linear Gene Expression Model As a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LGEM formulation matches the standard linear layer in neural networks, where the output is a linear transformation of the input plus a bias term.\n",
    "By combining $G$ and $W$ into a single matrix $M = G W$ with dimensions $n \\times K$, we can write the prediction for each perturbation as:\n",
    "$$\n",
    "y = M p^\\top + b\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $y$ is the predicted gene expression vector ($n \\times 1$).\n",
    "- $p^\\top$ is the transpose of the perturbation embedding vector ($K \\times 1$).\n",
    "- $M$ serves as the weight matrix in the neural network.\n",
    "- $b$ is the bias vector.\n",
    "\n",
    "This can directly be interpreted as the standard linear layer given by:\n",
    "$$\n",
    "y = W x + b\n",
    "$$\n",
    "\n",
    "Note that both models are linear.\n",
    "We can now choose to keep $G$ fixed (i.e., it consists of the top $K$ principal components from a PCA on $Y_{\\text{train}}$), and only learn $W$.\n",
    "Then, the neural network (_without activation functions_) is equivalent to the original LGEM.\n",
    "$M = G W$ would then be a combination of the fixed $G$ and the learned $W$.\n",
    "\n",
    "Options for improvement:\n",
    "- We can choose to set $G$ to be learnable.\n",
    "- We can include non-linearity (i.e., activation functions) in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that G, P, b, and Y_train are numpy arrays.\n",
    "G_np = np.array(G)  # Shape: (n_genes, K)\n",
    "P_np = np.array(P)  # Shape: (n_perturbations, K)\n",
    "b_np = np.array(b)  # Shape: (n_genes, 1)\n",
    "Y_train_np = np.array(Y_train)  # Shape: (n_genes, n_perturbations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert numpy arrays to torch tensors.\n",
    "# Note: By default tensors created from numpy arrays have requires_grad=False, which is\n",
    "# what we want for fixed G, P, and b.\n",
    "G_tensor = torch.from_numpy(G_np).float()\n",
    "P_tensor = torch.from_numpy(P_np).float()\n",
    "b_tensor = torch.from_numpy(b_np).float()\n",
    "Y_train_tensor = torch.from_numpy(Y_train_np).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "We create a custom PyTorch `nn.Module` where `W` is the only learnable parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LinearGeneExpressionModel(nn.Module):  # noqa: D101\n",
    "    def __init__(self, G, b):  # noqa: D107, N803\n",
    "        super(LinearGeneExpressionModel, self).__init__()\n",
    "        self.G = G\n",
    "        self.b = b\n",
    "        K = G.shape[1]  # noqa: N806\n",
    "        self.W = nn.Parameter(data=torch.randn(K, K))\n",
    "\n",
    "    def forward(self, P):  # noqa: D102, N803\n",
    "        M = self.G @ self.W  # noqa: N806\n",
    "        Y_pred = M @ P.T  # noqa: N806\n",
    "        Y_pred = Y_pred + self.b  # noqa: N806\n",
    "        return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up everything for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed=42)\n",
    "model = LinearGeneExpressionModel(G=G_tensor, b=b_tensor)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Only model parameters that have `requires_grad=True` will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter 'W' will be updated during training.\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Parameter '{name}' will be updated during training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create a dataset and dataloader.\n",
    "dataset = TensorDataset(P_tensor, Y_train_tensor.T)  # Transpose Y_train to match P.\n",
    "batch_size = P_tensor.shape[0]  # Use the full dataset.\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/20000], Loss: 2730787.5000\n",
      "Epoch [200/20000], Loss: 2173780.5000\n",
      "Epoch [300/20000], Loss: 1843706.7500\n",
      "Epoch [400/20000], Loss: 1622741.0000\n",
      "Epoch [500/20000], Loss: 1451387.0000\n",
      "Epoch [600/20000], Loss: 1303016.5000\n",
      "Epoch [700/20000], Loss: 1167628.1250\n",
      "Epoch [800/20000], Loss: 1042005.3750\n",
      "Epoch [900/20000], Loss: 925242.4375\n",
      "Epoch [1000/20000], Loss: 817101.1250\n",
      "Epoch [1100/20000], Loss: 717489.6250\n",
      "Epoch [1200/20000], Loss: 626303.7500\n",
      "Epoch [1300/20000], Loss: 543382.6250\n",
      "Epoch [1400/20000], Loss: 468498.9062\n",
      "Epoch [1500/20000], Loss: 401361.7812\n",
      "Epoch [1600/20000], Loss: 341620.5625\n",
      "Epoch [1700/20000], Loss: 288876.2812\n",
      "Epoch [1800/20000], Loss: 242690.8750\n",
      "Epoch [1900/20000], Loss: 202596.1094\n",
      "Epoch [2000/20000], Loss: 168103.8438\n",
      "Epoch [2100/20000], Loss: 138714.1250\n",
      "Epoch [2200/20000], Loss: 113924.0234\n",
      "Epoch [2300/20000], Loss: 93236.0469\n",
      "Epoch [2400/20000], Loss: 76165.3828\n",
      "Epoch [2500/20000], Loss: 62246.4141\n",
      "Epoch [2600/20000], Loss: 51038.9297\n",
      "Epoch [2700/20000], Loss: 42132.8828\n",
      "Epoch [2800/20000], Loss: 35152.4570\n",
      "Epoch [2900/20000], Loss: 29758.7676\n",
      "Epoch [3000/20000], Loss: 25651.4180\n",
      "Epoch [3100/20000], Loss: 22569.0371\n",
      "Epoch [3200/20000], Loss: 20288.4863\n",
      "Epoch [3300/20000], Loss: 18622.9609\n",
      "Epoch [3400/20000], Loss: 17419.4512\n",
      "Epoch [3500/20000], Loss: 16555.3223\n",
      "Epoch [3600/20000], Loss: 15934.5908\n",
      "Epoch [3700/20000], Loss: 15483.9043\n",
      "Epoch [3800/20000], Loss: 15148.6465\n",
      "Epoch [3900/20000], Loss: 14889.2119\n",
      "Epoch [4000/20000], Loss: 14677.6777\n",
      "Epoch [4100/20000], Loss: 14494.9268\n",
      "Epoch [4200/20000], Loss: 14328.2393\n",
      "Epoch [4300/20000], Loss: 14169.4160\n",
      "Epoch [4400/20000], Loss: 14013.3506\n",
      "Epoch [4500/20000], Loss: 13856.9316\n",
      "Epoch [4600/20000], Loss: 13698.3555\n",
      "Epoch [4700/20000], Loss: 13536.5947\n",
      "Epoch [4800/20000], Loss: 13371.0879\n",
      "Epoch [4900/20000], Loss: 13201.5303\n",
      "Epoch [5000/20000], Loss: 13027.7725\n",
      "Epoch [5100/20000], Loss: 12849.7471\n",
      "Epoch [5200/20000], Loss: 12667.4316\n",
      "Epoch [5300/20000], Loss: 12480.8369\n",
      "Epoch [5400/20000], Loss: 12289.9902\n",
      "Epoch [5500/20000], Loss: 12094.9434\n",
      "Epoch [5600/20000], Loss: 11895.7637\n",
      "Epoch [5700/20000], Loss: 11692.5117\n",
      "Epoch [5800/20000], Loss: 11485.2891\n",
      "Epoch [5900/20000], Loss: 11274.1963\n",
      "Epoch [6000/20000], Loss: 11059.3496\n",
      "Epoch [6100/20000], Loss: 10840.8799\n",
      "Epoch [6200/20000], Loss: 10618.9287\n",
      "Epoch [6300/20000], Loss: 10393.6582\n",
      "Epoch [6400/20000], Loss: 10165.2334\n",
      "Epoch [6500/20000], Loss: 9933.8428\n",
      "Epoch [6600/20000], Loss: 9699.6904\n",
      "Epoch [6700/20000], Loss: 9462.9912\n",
      "Epoch [6800/20000], Loss: 9223.9727\n",
      "Epoch [6900/20000], Loss: 8982.8867\n",
      "Epoch [7000/20000], Loss: 8739.9971\n",
      "Epoch [7100/20000], Loss: 8495.5820\n",
      "Epoch [7200/20000], Loss: 8249.9375\n",
      "Epoch [7300/20000], Loss: 8003.3691\n",
      "Epoch [7400/20000], Loss: 7756.2012\n",
      "Epoch [7500/20000], Loss: 7508.7700\n",
      "Epoch [7600/20000], Loss: 7261.4199\n",
      "Epoch [7700/20000], Loss: 7014.5054\n",
      "Epoch [7800/20000], Loss: 6768.3931\n",
      "Epoch [7900/20000], Loss: 6523.4487\n",
      "Epoch [8000/20000], Loss: 6280.0415\n",
      "Epoch [8100/20000], Loss: 6038.5400\n",
      "Epoch [8200/20000], Loss: 5799.3135\n",
      "Epoch [8300/20000], Loss: 5562.7183\n",
      "Epoch [8400/20000], Loss: 5329.1084\n",
      "Epoch [8500/20000], Loss: 5098.8184\n",
      "Epoch [8600/20000], Loss: 4872.1699\n",
      "Epoch [8700/20000], Loss: 4649.4668\n",
      "Epoch [8800/20000], Loss: 4430.9907\n",
      "Epoch [8900/20000], Loss: 4217.0059\n",
      "Epoch [9000/20000], Loss: 4007.7471\n",
      "Epoch [9100/20000], Loss: 3803.4265\n",
      "Epoch [9200/20000], Loss: 3604.2317\n",
      "Epoch [9300/20000], Loss: 3410.3228\n",
      "Epoch [9400/20000], Loss: 3221.8398\n",
      "Epoch [9500/20000], Loss: 3038.8938\n",
      "Epoch [9600/20000], Loss: 2861.5811\n",
      "Epoch [9700/20000], Loss: 2689.9758\n",
      "Epoch [9800/20000], Loss: 2524.1343\n",
      "Epoch [9900/20000], Loss: 2364.1001\n",
      "Epoch [10000/20000], Loss: 2209.9058\n",
      "Epoch [10100/20000], Loss: 2061.5737\n",
      "Epoch [10200/20000], Loss: 1919.1200\n",
      "Epoch [10300/20000], Loss: 1782.5564\n",
      "Epoch [10400/20000], Loss: 1651.8833\n",
      "Epoch [10500/20000], Loss: 1527.1038\n",
      "Epoch [10600/20000], Loss: 1408.2142\n",
      "Epoch [10700/20000], Loss: 1295.2042\n",
      "Epoch [10800/20000], Loss: 1188.0566\n",
      "Epoch [10900/20000], Loss: 1086.7458\n",
      "Epoch [11000/20000], Loss: 991.2341\n",
      "Epoch [11100/20000], Loss: 901.4697\n",
      "Epoch [11200/20000], Loss: 817.3856\n",
      "Epoch [11300/20000], Loss: 738.8962\n",
      "Epoch [11400/20000], Loss: 665.8945\n",
      "Epoch [11500/20000], Loss: 598.2542\n",
      "Epoch [11600/20000], Loss: 535.8262\n",
      "Epoch [11700/20000], Loss: 478.4409\n",
      "Epoch [11800/20000], Loss: 425.9067\n",
      "Epoch [11900/20000], Loss: 378.0140\n",
      "Epoch [12000/20000], Loss: 334.5349\n",
      "Epoch [12100/20000], Loss: 295.2269\n",
      "Epoch [12200/20000], Loss: 259.8362\n",
      "Epoch [12300/20000], Loss: 228.1004\n",
      "Epoch [12400/20000], Loss: 199.7526\n",
      "Epoch [12500/20000], Loss: 174.5240\n",
      "Epoch [12600/20000], Loss: 152.1492\n",
      "Epoch [12700/20000], Loss: 132.3682\n",
      "Epoch [12800/20000], Loss: 114.9305\n",
      "Epoch [12900/20000], Loss: 99.5972\n",
      "Epoch [13000/20000], Loss: 86.1437\n",
      "Epoch [13100/20000], Loss: 74.3615\n",
      "Epoch [13200/20000], Loss: 64.0590\n",
      "Epoch [13300/20000], Loss: 55.0623\n",
      "Epoch [13400/20000], Loss: 47.2154\n",
      "Epoch [13500/20000], Loss: 40.3792\n",
      "Epoch [13600/20000], Loss: 34.4311\n",
      "Epoch [13700/20000], Loss: 29.2634\n",
      "Epoch [13800/20000], Loss: 24.7821\n",
      "Epoch [13900/20000], Loss: 20.9050\n",
      "Epoch [14000/20000], Loss: 17.5604\n",
      "Epoch [14100/20000], Loss: 14.6855\n",
      "Epoch [14200/20000], Loss: 12.2246\n",
      "Epoch [14300/20000], Loss: 10.1289\n",
      "Epoch [14400/20000], Loss: 8.3536\n",
      "Epoch [14500/20000], Loss: 6.8601\n",
      "Epoch [14600/20000], Loss: 5.6118\n",
      "Epoch [14700/20000], Loss: 4.5768\n",
      "Epoch [14800/20000], Loss: 3.7254\n",
      "Epoch [14900/20000], Loss: 3.0312\n",
      "Epoch [15000/20000], Loss: 2.4704\n",
      "Epoch [15100/20000], Loss: 2.0209\n",
      "Epoch [15200/20000], Loss: 1.6646\n",
      "Epoch [15300/20000], Loss: 1.3845\n",
      "Epoch [15400/20000], Loss: 1.1663\n",
      "Epoch [15500/20000], Loss: 0.9978\n",
      "Epoch [15600/20000], Loss: 0.8688\n",
      "Epoch [15700/20000], Loss: 0.7705\n",
      "Epoch [15800/20000], Loss: 0.6961\n",
      "Epoch [15900/20000], Loss: 0.6400\n",
      "Epoch [16000/20000], Loss: 0.5977\n",
      "Epoch [16100/20000], Loss: 0.5661\n",
      "Epoch [16200/20000], Loss: 0.5415\n",
      "Epoch [16300/20000], Loss: 0.5230\n",
      "Epoch [16400/20000], Loss: 0.5086\n",
      "Epoch [16500/20000], Loss: 0.4976\n",
      "Epoch [16600/20000], Loss: 0.4884\n",
      "Epoch [16700/20000], Loss: 0.4810\n",
      "Epoch [16800/20000], Loss: 0.4749\n",
      "Epoch [16900/20000], Loss: 0.4697\n",
      "Epoch [17000/20000], Loss: 0.4652\n",
      "Epoch [17100/20000], Loss: 0.4613\n",
      "Epoch [17200/20000], Loss: 0.4576\n",
      "Epoch [17300/20000], Loss: 0.4555\n",
      "Epoch [17400/20000], Loss: 0.4561\n",
      "Epoch [17500/20000], Loss: 0.4488\n",
      "Epoch [17600/20000], Loss: 0.4464\n",
      "Epoch [17700/20000], Loss: 0.4472\n",
      "Epoch [17800/20000], Loss: 0.4420\n",
      "Epoch [17900/20000], Loss: 0.4399\n",
      "Epoch [18000/20000], Loss: 0.4419\n",
      "Epoch [18100/20000], Loss: 0.4366\n",
      "Epoch [18200/20000], Loss: 0.4351\n",
      "Epoch [18300/20000], Loss: 0.5758\n",
      "Epoch [18400/20000], Loss: 0.4327\n",
      "Epoch [18500/20000], Loss: 0.4318\n",
      "Epoch [18600/20000], Loss: 0.4308\n",
      "Epoch [18700/20000], Loss: 0.4303\n",
      "Epoch [18800/20000], Loss: 0.4294\n",
      "Epoch [18900/20000], Loss: 0.4288\n",
      "Epoch [19000/20000], Loss: 0.4304\n",
      "Epoch [19100/20000], Loss: 0.4279\n",
      "Epoch [19200/20000], Loss: 0.4276\n",
      "Epoch [19300/20000], Loss: 0.4275\n",
      "Epoch [19400/20000], Loss: 0.4271\n",
      "Epoch [19500/20000], Loss: 0.4271\n",
      "Epoch [19600/20000], Loss: 0.4268\n",
      "Epoch [19700/20000], Loss: 0.4295\n",
      "Epoch [19800/20000], Loss: 0.4266\n",
      "Epoch [19900/20000], Loss: 0.4266\n",
      "Epoch [20000/20000], Loss: 0.4266\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "num_epochs = 20000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_P, batch_Y in dataloader:  # noqa: N816\n",
    "        optimizer.zero_grad()\n",
    "        Y_pred = model(batch_P)\n",
    "        loss = criterion(Y_pred, batch_Y.T)  # Transpose back to (n_genes x batch_size).\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    train_losses.append(avg_epoch_loss)\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses)\n",
    "plt.xlim(0, num_epochs)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.4702258340548724e-05\n"
     ]
    }
   ],
   "source": [
    "# Compute the mean squared error between the W obtained via least squares optimization\n",
    "# and via learning.\n",
    "mse = np.mean((W - model.W.detach().numpy()) ** 2)\n",
    "print(f\"MSE: {mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
