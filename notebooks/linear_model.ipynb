{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pertdata as pt\n",
    "import torch\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess the scRNA-seq Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show available datasets from the `pertdata` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dict = pt.datasets()\n",
    "print(\"Available datasets:\")\n",
    "for key in datasets_dict.keys():\n",
    "    print(f\"  {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the \"NormanWeissman2019_filtered\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormanWeissman2019_filtered = pt.PertDataset(name=\"NormanWeissman2019_filtered\")\n",
    "print(NormanWeissman2019_filtered)\n",
    "adata = NormanWeissman2019_filtered.adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The expression matrix is in adata.X with genes as rows and cells as columns.\n",
    "- Perturbation labels are stored in `adata.obs['perturbation']`.\n",
    "- Gene names are in `adata.var_names`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that the data is in a dense format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sparse.issparse(adata.X):\n",
    "    adata.X = adata.X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate (pseudobulk) the single-cell data per perturbation condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbations = adata.obs[\"perturbation\"].unique()\n",
    "X_train = []\n",
    "\n",
    "for pert in perturbations:\n",
    "    # Subset the data for the current perturbation.\n",
    "    cells = adata[adata.obs[\"perturbation\"] == pert]\n",
    "    # Compute the average expression across cells.\n",
    "    mean_expression = cells.X.mean(axis=0)\n",
    "    X_train.append(mean_expression)\n",
    "\n",
    "# Convert Y_train to a numpy array\n",
    "Y_train = np.array(X_train).T  # Shape: (n_genes, n_perturbations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Linear Gene Expression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **Linear Gene Expression Model**, we have:\n",
    "- Embeddings for read-out genes: $G$ (an $n \\times K$ matrix, where $n$ is the number of read-out genes and $K$ is the dimensionality of the embeddings).\n",
    "- Embeddings for perturbations: $P$ (an $m \\times K$ matrix, where $m$ is the number of perturbations).\n",
    "- Weight matrix: $W$ (a $K \\times K$ matrix to be learned).\n",
    "- Bias vector: $b$ (an $n \\times 1$ vector of average gene expressions).\n",
    "\n",
    "The model predicts gene expression values using:\n",
    "$$\n",
    "Y_{\\text{train}} \\approx G W P^\\top + b\n",
    "$$\n",
    "\n",
    "Note: The bias vector $b$ (with dimensions $n \\times 1$) is added to each column of the matrix $G W P^\\top$ (with dimensions $n \\times m$).\n",
    "This operation effectively _broadcasts_ the vector $b$ across all $m$ columns, repeating it $m$ times to match the dimensions of $G W P^\\top$.\n",
    "PyTorch handles broadcasting automatically implicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulating the Linear Gene Expression Model As a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LGEM formulation matches the standard linear layer in neural networks, where the output is a linear transformation of the input plus a bias term.\n",
    "By combining $G$ and $W$ into a single matrix $M = G W$ with dimensions $n \\times K$, we can write the prediction for each perturbation as:\n",
    "\n",
    "$$\n",
    "y = M p^\\top + b\n",
    "$$.\n",
    "\n",
    "Here:\n",
    "- $y$ is the predicted gene expression vector ($n \\times 1$).\n",
    "- $p^\\top$ is the transpose of the perturbation embedding vector ($K \\times 1$).\n",
    "- $M$ serves as the weight matrix in the neural network.\n",
    "- $b$ is the bias vector.\n",
    "\n",
    "This can directly be interpreted as the standard linear layer given by:\n",
    "\n",
    "$$\n",
    "y = W x + b\n",
    "$$.\n",
    "\n",
    "Since both models are linear and $G$ is fixed, the neural network _without activation functions_ is equivalent to the original LGEM.\n",
    "\n",
    "We can now choose to keep $G$ fixed (i.e., it consists of the top $K$ principal components from a PCA on $Y_{\\text{train}}$), and only learn $W$.\n",
    "$M = G W$ would then be a combination of the fixed $G$ and the learned $W$.\n",
    "Or we can choose to set $G$ to be learnable.\n",
    "\n",
    "Overview:\n",
    "1.\tFixed $G$:\n",
    "\t•\tModel Equation:  y = (G W) x + b \n",
    "\t•\tParameters Learned:  W ,  b \n",
    "\t•\tEquivalence: Maintained with the original model.\n",
    "2.\tLearnable  G :\n",
    "\t•\tModel Equation:  y = (G W) x + b \n",
    "\t•\tParameters Learned:  G ,  W ,  b \n",
    "\t•\tEquivalence: Model becomes more flexible but differs from the original.\n",
    "3. Non-Linearity\n",
    "4. Learnable G & Non-Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LinearGeneExpressionModel(nn.Module):  # noqa: D101\n",
    "    def __init__(self, G, b):  # noqa: D107\n",
    "        super(LinearGeneExpressionModel, self).__init__()\n",
    "        self.G = G  # Fixed gene embeddings (n_genes x K)\n",
    "        self.b = b  # Fixed bias vector (n_genes x 1)\n",
    "        K = G.shape[1]  # Dimensionality of embeddings\n",
    "        self.W = nn.Parameter(torch.randn(K, K))  # Learnable weight matrix\n",
    "\n",
    "    def forward(self, P):  # noqa: D102\n",
    "        # P: Perturbation embeddings (n_perturbations x K)\n",
    "        GW = self.G @ self.W  # (n_genes x K)\n",
    "        Y_pred = GW @ P.T  # (n_genes x n_perturbations)\n",
    "        Y_pred = Y_pred + self.b  # Broadcasting b across columns\n",
    "        return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with fixed G and b\n",
    "model = LinearGeneExpressionModel(G=G_tensor, b=b_tensor)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Only model.parameters() that have requires_grad=True (i.e.,  W ) will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000  # Set the number of training epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    Y_pred = model(P_tensor)  # Y_pred: (n_genes x n_perturbations)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(Y_pred, Y_train_tensor)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    Y_pred = model(P_tensor)\n",
    "    final_loss = criterion(Y_pred, Y_train_tensor)\n",
    "    print(f\"Final Training Loss: {final_loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
