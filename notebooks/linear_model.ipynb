{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess the scRNA-seq Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the \"NormanWeissman2019_filtered\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pertdata as pt  # type: ignore\n",
    "\n",
    "NormanWeissman2019_filtered = pt.PertDataset(\n",
    "    name=\"NormanWeissman2019_filtered\",\n",
    "    cache_dir_path=os.path.join(\"..\", \".pertdata_cache\"),\n",
    "    silent=False,\n",
    ")\n",
    "print(NormanWeissman2019_filtered)\n",
    "adata = NormanWeissman2019_filtered.adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the numbers of unique perturbations and genes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbations = adata.obs[\"perturbation\"].unique()\n",
    "genes = adata.var_names\n",
    "print(f\"Unique perturbations: {len(perturbations)}\")\n",
    "print(f\"Unique genes: {len(genes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demultiplex `adata` for control/single/double perturbations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_control = adata[adata.obs[\"nperts\"] == 0].copy()\n",
    "adata_single = adata[adata.obs[\"nperts\"] == 1].copy()\n",
    "adata_double = adata[adata.obs[\"nperts\"] == 2].copy()\n",
    "\n",
    "perturbations_control = adata_control.obs[\"perturbation\"].unique()\n",
    "perturbations_single = adata_single.obs[\"perturbation\"].unique()\n",
    "perturbations_double = adata_double.obs[\"perturbation\"].unique()\n",
    "\n",
    "print(f\"Unique control perturbations: {len(perturbations_control)}\")\n",
    "print(f\"Unique single perturbations: {len(perturbations_single)}\")\n",
    "print(f\"Unique double perturbations: {len(perturbations_double)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue working **with single-gene perturbations only**.\n",
    "Also, we remove perturbations that are not measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = []\n",
    "for perturbation in perturbations_single:\n",
    "    if perturbation not in adata_single.var_names:\n",
    "        to_remove.append(perturbation)\n",
    "\n",
    "print(f\"Removing {len(to_remove)} perturbations: {to_remove}\")\n",
    "\n",
    "adata_single_filtered = adata_single[~adata_single.obs[\"perturbation\"].isin(to_remove)].copy()\n",
    "\n",
    "perturbations_single_filtered = adata_single_filtered.obs[\"perturbation\"].unique()\n",
    "print(f\"Unique single filtered perturbations: {len(perturbations_single_filtered)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that the expression matrix is in a dense format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import issparse  # type: ignore\n",
    "\n",
    "if issparse(adata_single_filtered):\n",
    "    adata_single_filtered.X = adata_single_filtered.X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate (pseudobulk) the single-cell data per perturbation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = []\n",
    "\n",
    "for perturbation in perturbations_single_filtered:\n",
    "    # Subset the data for the current perturbation.\n",
    "    cells = adata_single_filtered[adata_single_filtered.obs[\"perturbation\"] == perturbation]\n",
    "\n",
    "    # Compute the average expressions across cells.\n",
    "    mean_expressions = cells.X.mean(axis=0)\n",
    "\n",
    "    # Append the mean expression to Y_train.\n",
    "    Y_train.append(mean_expressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert `Y_train` to a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # type: ignore\n",
    "\n",
    "Y_train = np.array(Y_train).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Linear Gene Expression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Linear Gene Expression Model (LGEM) by [Ahlmann-Eltze et al. (2024)](https://doi.org/10.1101/2024.09.16.613342), we have:\n",
    "- Embeddings for read-out genes: $G$ (an $n \\times K$ matrix, where $n$ is the number of read-out genes and $K$ is the dimensionality of the embeddings).\n",
    "- Embeddings for perturbations: $P$ (an $m \\times K$ matrix, where $m$ is the number of perturbations).\n",
    "\n",
    "Given a data matrix $Y_{\\text{train}}$ of gene expression values, the model then fits the matrix $W$ by minimizing:\n",
    "$$\n",
    "\\arg\\min_{W} \\| Y_{\\text{train}} - (G W P^\\top + b) \\|^2\n",
    "$$\n",
    "\n",
    "Hence, we furthermore have:\n",
    "- Data matrix: $Y_{\\text{train}}$ (an $n \\times m$ matrix, i.e., pseudobulked per condition/perturbation).\n",
    "- Weight matrix: $W$ (a $K \\times K$ matrix to be learned).\n",
    "- Bias vector: $b$ (an $n \\times 1$ vector of average gene expressions).\n",
    "\n",
    "The model then predicts gene expression values using:\n",
    "$$\n",
    "Y_{\\text{train}} \\approx G W P^\\top + b\n",
    "$$\n",
    "\n",
    "Note: The bias vector $b$ (with dimensions $n \\times 1$) is added to each column of the matrix $G W P^\\top$ (with dimensions $n \\times m$).\n",
    "This operation effectively _broadcasts_ the vector $b$ across all $m$ columns, repeating it $m$ times to match the dimensions of $G W P^\\top$.\n",
    "PyTorch handles broadcasting automatically implicitly.\n",
    "\n",
    "In the paper, to obtain the embeddings $G$ and $P$, they performed a PCA on $Y_{\\text{train}}$ and used the top $K$ principal components for $G$.\n",
    "They then subset this $G$ to only the rows corresponding to genes that have been perturbed in the training data (and hence appear as columns in $Y_{\\text{train}}$) and used the resulting matrix for $P$.\n",
    "When using the model for prediction, they replace $P$ with the matrix formed by the rows of $G$ corresponding to genes perturbed in the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the expression matrix $Y_{\\text{train}}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_copy = Y_train.copy()\n",
    "Y_train = Y_train.T  # Shape: (n_genes, n_perturbations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a PCA on $Y_{\\text{train}}$ to obtain the top $K$ principal components, which will serve as the gene embeddings $G$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA  # type: ignore\n",
    "\n",
    "K = 10\n",
    "pca = PCA(n_components=K)\n",
    "G = pca.fit_transform(Y_train)  # Shape: (n_genes, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract perturbation embeddings $P$ from $G$ by subsetting $G$ to only the rows corresponding to genes that have been perturbed in the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the indexes of perturbations.\n",
    "indexes = [list(adata_single_filtered.var_names).index(gene_name) for gene_name in perturbations_single_filtered]\n",
    "\n",
    "# Extract P from G.\n",
    "P = G[indexes, :]  # Shape: (n_perturbations, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $b$ as the average expression of each gene across all perturbations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Y_train.mean(axis=1, keepdims=True)  # Shape: (n_genes, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Center $Y_{\\text{train}}$ by subtracting $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_centered = Y_train - b  # Shape: (n_genes, n_perturbations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can vectorize the equation $Y_{\\text{centered}} = G W P^\\top$ and set it up in a form suitable for least squares.\n",
    "\n",
    "Using the mixed-product property of the Kronecker product, we have:\n",
    "$$\n",
    "\\text{vec} (Y_{\\text{centered}}) = (P \\otimes G) \\text{vec} (W)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $\\text{vec} (Y_{\\text{centered}})$ is the vectorization of $Y_{\\text{centered}}$ (flattened column-wise).\n",
    "- $P \\otimes G$ is the Kronecker product of $P$ and $G$.\n",
    "- $\\text{vec} (W)$ is the vectorization of $W$ (flattened column-wise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import kron  # type: ignore\n",
    "\n",
    "# Flatten Y_centered.\n",
    "Y_vec = Y_centered.flatten(order=\"F\")\n",
    "\n",
    "# Ensure that P and G are numpy arrays.\n",
    "P = np.array(P)\n",
    "G = np.array(G)\n",
    "\n",
    "# Compute the Kronecker product of P and G.\n",
    "Kron_P_G = kron(P, G)  # Shape: (n_genes * n_perturbations, K * K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now solve the linear equation:\n",
    "$$\n",
    "Y_{\\text{vec}} = (P \\otimes G) \\cdot W_{\\text{vec}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import lstsq  # type: ignore\n",
    "\n",
    "# Solve the least squares problem.\n",
    "W_vec, residuals, rank, s = lstsq(Kron_P_G, Y_vec, rcond=None)\n",
    "\n",
    "# Reshape vec(W) back to matrix W.\n",
    "W = W_vec.reshape(K, K, order=\"F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct Y_centered using G, W, and P.\n",
    "Y_centered_pred = G @ W @ P.T  # Shape: (n_genes, n_perturbations)\n",
    "\n",
    "# Compute the mean squared error.\n",
    "mse = np.mean((Y_centered - Y_centered_pred) ** 2)\n",
    "print(f\"MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulating the Linear Gene Expression Model As a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LGEM formulation matches the standard linear layer in neural networks, where the output is a linear transformation of the input plus a bias term.\n",
    "By combining $G$ and $W$ into a single matrix $M = G W$ with dimensions $n \\times K$, we can write the prediction for each perturbation as:\n",
    "$$\n",
    "y = M p^\\top + b\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $y$ is the predicted gene expression vector ($n \\times 1$).\n",
    "- $p^\\top$ is the transpose of the perturbation embedding vector ($K \\times 1$).\n",
    "- $M$ serves as the weight matrix in the neural network.\n",
    "- $b$ is the bias vector.\n",
    "\n",
    "This can directly be interpreted as the standard linear layer given by:\n",
    "$$\n",
    "y = W x + b\n",
    "$$\n",
    "\n",
    "Note that both models are linear.\n",
    "We can now choose to keep $G$ fixed (i.e., it consists of the top $K$ principal components from a PCA on $Y_{\\text{train}}$), and only learn $W$.\n",
    "Then, the neural network (_without activation functions_) is equivalent to the original LGEM.\n",
    "$M = G W$ would then be a combination of the fixed $G$ and the learned $W$.\n",
    "\n",
    "Options for improvement:\n",
    "- We can choose to set $G$ to be learnable.\n",
    "- We can include non-linearity (i.e., activation functions) in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that G, P, b, and Y_train are numpy arrays.\n",
    "G_np = np.array(G)  # Shape: (n_genes, K)\n",
    "P_np = np.array(P)  # Shape: (n_perturbations, K)\n",
    "b_np = np.array(b)  # Shape: (n_genes, 1)\n",
    "Y_train_np = np.array(Y_train)  # Shape: (n_genes, n_perturbations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # type: ignore\n",
    "\n",
    "# Convert numpy arrays to torch tensors.\n",
    "# Note: By default tensors created from numpy arrays have requires_grad=False, which is\n",
    "# what we want for fixed G, P, and b.\n",
    "G_tensor = torch.from_numpy(G_np).float()\n",
    "P_tensor = torch.from_numpy(P_np).float()\n",
    "b_tensor = torch.from_numpy(b_np).float()\n",
    "Y_train_tensor = torch.from_numpy(Y_train_np).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "We create a custom PyTorch `nn.Module` where `W` is the only learnable parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn  # type: ignore\n",
    "\n",
    "\n",
    "class LinearGeneExpressionModel(nn.Module):  # noqa: D101\n",
    "    def __init__(self, G, b):  # noqa: D107, N803\n",
    "        super(LinearGeneExpressionModel, self).__init__()\n",
    "        self.G = G\n",
    "        self.b = b\n",
    "        K = G.shape[1]  # noqa: N806\n",
    "        self.W = nn.Parameter(data=torch.randn(K, K))\n",
    "\n",
    "    def forward(self, P):  # noqa: D102, N803\n",
    "        M = self.G @ self.W  # noqa: N806\n",
    "        Y_pred = M @ P.T  # noqa: N806\n",
    "        Y_pred = Y_pred + self.b  # noqa: N806\n",
    "        return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up everything for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed=42)\n",
    "model = LinearGeneExpressionModel(G=G_tensor, b=b_tensor)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Only model parameters that have `requires_grad=True` will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Parameter '{name}' will be updated during training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset  # type: ignore\n",
    "\n",
    "# Create a dataset and dataloader.\n",
    "dataset = TensorDataset(P_tensor, Y_train_tensor.T)  # Transpose Y_train to match P.\n",
    "batch_size = P_tensor.shape[0]  # Use the full dataset.\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "num_epochs = 20000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_P, batch_Y in dataloader:  # noqa: N816\n",
    "        optimizer.zero_grad()\n",
    "        Y_pred = model(batch_P)\n",
    "        loss = criterion(Y_pred, batch_Y.T)  # Transpose back to (n_genes x batch_size).\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    train_losses.append(avg_epoch_loss)\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # type: ignore\n",
    "\n",
    "plt.plot(train_losses)\n",
    "plt.xlim(0, num_epochs)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean squared error between the W obtained via least squares optimization\n",
    "# and via learning.\n",
    "mse = np.mean((W - model.W.detach().numpy()) ** 2)\n",
    "print(f\"MSE: {mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
